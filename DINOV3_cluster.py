import os
import shutil
import pandas as pd
import numpy as np
from tqdm import tqdm

import scipy.sparse as sp
from sklearn.metrics.pairwise import cosine_similarity
from sklearn.preprocessing import StandardScaler

import igraph as ig
import leidenalg

# =========================================================
# é…ç½®ï¼ˆåâ€œç²—èšç±»â€çš„å®‰å…¨å‚æ•°ï¼‰
# =========================================================
CSV_PATH = "all_features_dinov3.csv"
IMAGE_ROOT = os.path.abspath("all_cutouts")
OUTPUT_FOLDER = "all_leiden_init"

TOPK = 30                  # Mutual kNN
# SECOND_ORDER_WEIGHT = 0.15  # äºŒé˜¶æ‰©æ•£ï¼ˆä¿è¯ä¼ é€’ï¼‰
# RESOLUTION = 1.8           # â­ Leiden åˆ†è¾¨ç‡ï¼ˆè¶Šå°è¶Šç²—ï¼Œ0.4~0.8 æ¨èï¼‰

SECOND_ORDER_WEIGHT = 0.05  # äºŒé˜¶æ‰©æ•£ï¼ˆä¿è¯ä¼ é€’ï¼‰
RESOLUTION = 2.5           # â­ Leiden åˆ†è¾¨ç‡ï¼ˆè¶Šå°è¶Šç²—ï¼Œ0.4~0.8 æ¨èï¼‰

os.makedirs(OUTPUT_FOLDER, exist_ok=True)

# =========================================================
# 1. è¯»å–ç‰¹å¾
# =========================================================
df = pd.read_csv(CSV_PATH)
if "filename" not in df.columns:
    raise ValueError("CSV ä¸­å¿…é¡»åŒ…å« 'filename' åˆ—")

print(f"âœ… è¯»å– {len(df)} æ¡ç‰¹å¾è®°å½•")

# =========================================================
# 2. ä¸»ç¼–å·ï¼ˆåˆå¹¶ exterior / interiorï¼‰
# =========================================================
def get_piece_id(filename):
    name = os.path.splitext(filename)[0]
    name = name.replace("_exterior", "").replace("_interior", "")
    return name.lower()

df["main_id"] = df["filename"].apply(get_piece_id)

# =========================================================
# 3. æ¯ä¸ªé™¶ç‰‡åªä¿ç•™ä¸¤å¼ ï¼ˆæ­£åï¼‰
# =========================================================
def select_two_images(group):
    if len(group) < 2:
        return pd.DataFrame([])
    return group.iloc[:2]

selected_df = (
    df.groupby("main_id", group_keys=False)
      .apply(select_two_images)
      .reset_index(drop=True)
)

num_pieces = selected_df["main_id"].nunique()
print(f"âœ… è¿‡æ»¤åé™¶ç‰‡æ•°: {num_pieces}")

# =========================================================
# 4. ç‰¹å¾åˆ—
# =========================================================
feature_cols = [c for c in df.columns if c not in ["filename", "main_id"]]

# =========================================================
# 5. æ­£åé¢ç‰¹å¾èåˆï¼ˆæ‹¼æ¥ï¼Œä¿ç•™åˆ¤åˆ«æ€§ï¼‰
# =========================================================
def fuse_features(group):
    v1 = group.iloc[0][feature_cols].values
    v2 = group.iloc[1][feature_cols].values
    fused = np.concatenate([v1,v2])
    return pd.Series(fused, name=group["main_id"].iloc[0])

merged_features = (
    selected_df.groupby("main_id", group_keys=False)
    .apply(fuse_features)
)

features = np.stack(merged_features.values).astype(np.float32)
features = StandardScaler().fit_transform(features)

piece_ids = merged_features.index.to_numpy()
n = features.shape[0]

print(f"âœ… èåˆåç‰¹å¾çŸ©é˜µ: {features.shape}")

# =========================================================
# 6. Mutual kNN å›¾ï¼ˆè¿ç»­æƒé‡ï¼‰
# =========================================================
print("ğŸ”— æ„å»º Mutual kNN ç›¸ä¼¼åº¦å›¾...")

sim = cosine_similarity(features)
np.fill_diagonal(sim, 0)

neighbors = np.argsort(sim, axis=1)[:, -TOPK:]

rows, cols, data = [], [], []

for i in tqdm(range(n), desc="Building mutual kNN graph"):
    for j in neighbors[i]:
        if i in neighbors[j]:
            w = sim[i, j] ** 2   # æŠ‘åˆ¶å¼±è¾¹
            rows.append(i)
            cols.append(j)
            data.append(w)

A = sp.csr_matrix(
    (data, (rows, cols)),
    shape=(n, n),
    dtype=np.float32
)

print(f"âœ… ä¸€é˜¶å›¾ï¼šèŠ‚ç‚¹ {n}ï¼Œè¾¹æ•° {A.nnz}")

# =========================================================
# 7. äºŒé˜¶æ‰©æ•£ï¼ˆå¢å¼ºä¼ é€’æ€§ï¼Œä½†ä¸è¿‡åº¦ï¼‰
# =========================================================
print("ğŸŒŠ äºŒé˜¶æ‰©æ•£å¢å¼º...")

A2 = A @ A
A2 = A2.multiply(SECOND_ORDER_WEIGHT)

# è½»é‡å‰ªæï¼Œé˜²æ­¢çˆ†ç‚¸
A2.data[A2.data < 0.01] = 0
A2.eliminate_zeros()

A_final = A + A2
A_final.eliminate_zeros()

print(f"âœ… æœ€ç»ˆå›¾ï¼šè¾¹æ•° {A_final.nnz}")

# =========================================================
# 8. Leiden ç¤¾åŒºå‘ç°ï¼ˆåˆå§‹ç²—èšç±»ï¼‰
# =========================================================
print("ğŸ§  æ‰§è¡Œ Leiden åˆå§‹èšç±»...")

edges = list(zip(A_final.nonzero()[0], A_final.nonzero()[1]))
weights = A_final.data.tolist()

g = ig.Graph(n=n, edges=edges, directed=False)
g.es["weight"] = weights

partition = leidenalg.find_partition(
    g,
    leidenalg.RBConfigurationVertexPartition,
    weights="weight",
    resolution_parameter=RESOLUTION
)

clusters = partition
print(f"ğŸ¯ Leiden å¾—åˆ° {len(clusters)} ä¸ªåˆå§‹èšç±»")

# =========================================================
# 9. piece_id â†’ cluster_id
# =========================================================
piece_to_cluster = {}
for cid, cluster in enumerate(clusters):
    for node_idx in cluster:
        piece_to_cluster[piece_ids[node_idx]] = cid

# =========================================================
# 10. åˆ›å»ºè¾“å‡ºç›®å½•
# =========================================================
for cid in range(len(clusters)):
    os.makedirs(os.path.join(OUTPUT_FOLDER, f"cluster_{cid}"), exist_ok=True)

# =========================================================
# 11. æŒ‰èšç±»å¤åˆ¶å›¾åƒ
# =========================================================
print("ğŸ“ æ­£åœ¨å¤åˆ¶å›¾åƒåˆ°å¯¹åº” cluster æ–‡ä»¶å¤¹...")

for piece_id, cluster_id in tqdm(piece_to_cluster.items()):
    files = selected_df[selected_df["main_id"] == piece_id]["filename"].values
    for f in files:
        src = os.path.join(IMAGE_ROOT, f)
        dst = os.path.join(OUTPUT_FOLDER, f"cluster_{cluster_id}", f)
        try:
            shutil.copy2(src, dst)
        except Exception as e:
            print(f"âš ï¸ æ— æ³•å¤åˆ¶ {f}: {e}")

print("ğŸ‰ Leiden åˆå§‹èšç±»å®Œæˆï¼")
