import os
import torch
import numpy as np
import pandas as pd
from PIL import Image, UnidentifiedImageError
from tqdm import tqdm
from torchvision import transforms

from model import DINOv3_S_Encoder   # ä½ çš„æ¨¡å‹å®šä¹‰


# =========================
# é…ç½®
# =========================
IMAGE_FOLDER = os.path.abspath("all_cutouts")
OUTPUT_CSV_PATH = "all_features_dinov3.csv"

WEIGHT_PATH = "dinov3_epoch_100.pth"   # â­ ä½ è®­ç»ƒå¥½çš„æ¨¡å‹
DEVICE = torch.device("cuda:0" if torch.cuda.is_available() else "cpu")

TARGET_SIZE = 224
MARGIN_RATIO = 0.08


# =========================
# å›¾åƒé¢„å¤„ç†ï¼ˆä¸ä½ è®­ç»ƒæ—¶ä¸€è‡´ï¼‰
# =========================
transform = transforms.Compose([
    transforms.ToTensor(),
    transforms.Normalize(
        mean=(0.485, 0.456, 0.406),
        std=(0.229, 0.224, 0.225)
    )
])


# =========================
# å‰æ™¯è£å‰ª + padding
# =========================
def crop_and_pad_foreground(image_rgba, target_size=224, margin_ratio=0.08):
    rgba = np.array(image_rgba)
    alpha = rgba[:, :, 3] / 255.0
    mask = alpha > 0

    ys, xs = np.where(mask)
    if len(xs) == 0:
        return None

    H, W = alpha.shape
    x_min, x_max = xs.min(), xs.max()
    y_min, y_max = ys.min(), ys.max()

    pad = int(margin_ratio * max(x_max - x_min, y_max - y_min))
    x_min = max(0, x_min - pad)
    y_min = max(0, y_min - pad)
    x_max = min(W - 1, x_max + pad)
    y_max = min(H - 1, y_max + pad)

    rgb = rgba[y_min:y_max+1, x_min:x_max+1, :3].astype(np.float32)
    alpha_crop = alpha[y_min:y_max+1, x_min:x_max+1]

    rgb *= alpha_crop[..., None]

    pil = Image.fromarray(np.uint8(np.clip(rgb, 0, 255)))

    w, h = pil.size
    scale = min(target_size / w, target_size / h)
    new_w, new_h = int(w * scale), int(h * scale)

    resized = pil.resize((new_w, new_h), Image.LANCZOS)

    canvas = Image.new("RGB", (target_size, target_size), (0, 0, 0))
    left = (target_size - new_w) // 2
    top = (target_size - new_h) // 2
    canvas.paste(resized, (left, top))

    return canvas


# =========================
# ä¸»æµç¨‹
# =========================
def extract_features():
    print(f"âœ… ä½¿ç”¨è®¾å¤‡: {DEVICE}")

    # --- åŠ è½½æ¨¡å‹ ---
    model = DINOv3_S_Encoder(
        weight_path=WEIGHT_PATH,
        proj_dim=128,
        train_backbone=False   # â­ æ¨ç†é˜¶æ®µå¿…é¡» False
    )
    model.load_state_dict(torch.load(WEIGHT_PATH, map_location="cpu"), strict=True)
    model = model.to(DEVICE)
    model.eval()

    image_files = [
        f for f in os.listdir(IMAGE_FOLDER)
        if f.lower().endswith((".png", ".jpg", ".jpeg"))
    ]

    all_feats = []
    all_names = []

    for file in tqdm(image_files, desc="æå– DINOv3 ç‰¹å¾"):
        img_path = os.path.join(IMAGE_FOLDER, file)

        try:
            image = Image.open(img_path).convert("RGBA")
        except UnidentifiedImageError:
            print(f"âš ï¸ æ— æ³•è¯†åˆ« {file}ï¼Œè·³è¿‡")
            continue

        processed = crop_and_pad_foreground(image, TARGET_SIZE, MARGIN_RATIO)
        if processed is None:
            print(f"âš ï¸ {file} æ— å‰æ™¯ï¼Œè·³è¿‡")
            continue

        x = transform(processed).unsqueeze(0).to(DEVICE)

        with torch.no_grad():
            _, z = model(x)     # â­ ç”¨ projector è¾“å‡º
            feat = z.squeeze(0).cpu().numpy()   # [128]

        all_feats.append(feat)
        all_names.append(file)

    if not all_feats:
        print("âŒ æ²¡æœ‰æˆåŠŸæå–ä»»ä½•ç‰¹å¾")
        return

    feats = np.stack(all_feats, axis=0)

    df = pd.DataFrame(feats)
    df.insert(0, "filename", all_names)
    df.to_csv(OUTPUT_CSV_PATH, index=False)

    print(f"\nâœ… å·²æå– {len(all_names)} å¼ å›¾åƒçš„ DINOv3 ç‰¹å¾")
    print(f"ğŸ“„ ç‰¹å¾ç»´åº¦: {feats.shape[1]}")
    print(f"ğŸ’¾ ä¿å­˜åˆ°: {OUTPUT_CSV_PATH}")


# =========================
# å¯åŠ¨
# =========================
if __name__ == "__main__":
    extract_features()
